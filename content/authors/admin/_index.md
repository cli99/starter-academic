---
# Display name
title: Cheng Li
# title: Cheng Li (李程)

# Is this the primary user of the site?
superuser: true

# Role/position
role: Member of Technical Staff

# Organizations/Affiliations
organizations:
- name: Black Forest Labs
  url: ""

# Short bio (displayed in user profile at end of posts)
bio: I specialize in building efficient AI training and inference systems using GPUs, with a focus on optimizing performance for Large Language Models (LLMs) and Large Vision Models (LVMs).

interests:
- Large Language Models
- System Optimization and Engineering for Deep Learning
- GPU and Parallel Computing

education:
  courses:
    - course: PhD in Computer Science
      institution: University of Illinois Urbana-Champaign
      year: 2020
    - course: MS in Computer Science and Engineering
      institution: University of Michigan
      year: 2015
    - course: BS in Computer Engineering
      institution: University of Michigan
      year: 2013
    - course: BS in Electrical Engineering
      institution: Shanghai Jiao Tong University
      year: 2013

# Social/Academic Networking
# For available icons, see: https://sourcethemes.com/academic/docs/page-builder/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "#contact" for contact widget.
social:
- icon: envelope
  icon_pack: fas
  link: '/#contact'  # For a direct email link, use "mailto:test@example.org".
# - icon: twitter
#   icon_pack: fab
#   link: https://twitter.com/GeorgeCushen
- icon: google-scholar
  icon_pack: ai
  link: https://scholar.google.com/citations?user=da9Vl6QAAAAJ&hl
- icon: github
  icon_pack: fab
  link: https://github.com/cli99
- icon: linkedin
  icon_pack: fab
  link: https://linkedin.com/in/cli99
# Link to a PDF of your resume/CV from the About widget.
# To enable, copy your resume/CV to `static/files/cv.pdf` and uncomment the lines below.
# - icon: cv
#   icon_pack: ai
#   link: pdf/Cheng_Li_CV.pdf

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: ""

# Highlight the author in author lists? (true/false)
highlight_name: true

# Organizational groups that you belong to (for People widget)
#   Set this to `[]` or comment out if you are not using People widget.
user_groups:
- Researchers
- Visitors
---

I am a Member of Technical Staff at [Black Forest Labs](https://blackforestlabs.ai/), specializing in optimizing the training and inference efficiency of Large Language Models (LLMs) and Large Vision Models (LVMs).

Previously, I worked at [Databricks Mosaic AI](https://www.databricks.com/product/machine-learning), where I played a key role in developing the [DBRX model](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) by optimizing memory utilization, computational efficiency, and communication strategies during training to achieve state-of-the-art performance ([Building DBRX-class Custom LLMs with Mosaic AI Training](https://www.databricks.com/blog/mosaic-ai-training-capabilities)). I collaborated with NVIDIA to resolve FP8 training challenges in TransformerEngine, enabling FP8 training for Mosaic AI models. Additionally, I led the technical effort to optimize the inference of Llama and DBRX models.

Prior to Databricks, I was part of [Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed), where I enhanced the performance and usability of LLMs in production systems such as [GitHub Copilot](https://github.com/features/copilot) and [DALL·E2](https://openai.com/index/dall-e-2/). My work included developing cutting-edge AI system technologies and scaling Microsoft DeepSpeed into a leading AI framework.

I created [llm-analysis](https://github.com/cli99/llm-analysis), an open-source tool for analyzing latency and memory in transformer models, helping with resource planning and optimization. Check it out!


<!-- I am a PhD candidate in Computer Science at the University of Illinois at Urbana-Champaign (UIUC) and a member of the [IMPACT Research Group](http://impact.crhc.illinois.edu/) led by Professor [Wen-Mei Hwu](https://ece.illinois.edu/directory/profile/w-hwu). -->


<!-- I am a senior software engineer at Databricks GenAI. My work has focused on optimizing training/inference of Deep Learning (DL) models, particularly on Large Language models (LLMs).

At Databricks, I have worked on building [DBRX](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) and optimizing its training performance (three months of training on 3072 H100 GPUs). I have aggressively optimized the memory usage/computation/communication to achieve SOTA training efficiency. Refer to [Building DBRX-class Custom LLMs with Mosaic AI Training](https://www.databricks.com/blog/mosaic-ai-training-capabilities) for more details.
Currently I am optimizing Llama3 and DBRX inference performance.

Before joining Databricks, I was a senior researcher at Microsoft,  where I worked on improving LLM/LMM performance/usability in production ([GitHub Copilot](https://github.com/features/copilot), [DALL·E2](https://openai.com/index/dall-e-2/), etc.), creating SOTA AI system technologies and building up [Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed), an open-source library that enables unprecedented scale and speed for training and inference.

I developed and open sourced [llm-analysis](https://github.com/cli99/llm-analysis): Latency and Memory Analysis of Transformer Models for Training and Inference. It helps planning resources for training/inference and suggests optimization opportunities. Check it out! -->
<!-- I received my PhD in CS from University of Illinois at Urbana-Champaign. During my PhD, I developed a number of [open-source tools](https://github.com/rai-project) to benchmark, profile, and summarize DL training and inference across hardware and software stacks. The tools have been used to inform system design for DL model serving and develop highly tuned GPU kernels for model inference. -->


<!-- [**I'm joining Microsoft in August！**](mailto:cli99@illinois.edu) -->

<!-- Currently I am working on [MLModelScope (CarML)](http://mlmodelscope.org/) as part of the IBM-ILLINOIS [Center for Cognitive Computing Systems Research (C3SR)](https://www.c3sr.com/). MLModelScope is an open-source, framework and hardware agnostic, extensible and customizable platform for evaluating and profiling ML models across datasets / frameworks / systems, at scale and across stack. MLModelScope is collaborating with the [MLPerf](https://mlperf.org/) community with the goal for it to be the "to-go" platform for Machine Learning inference benchmarking. -->

<!--
Cheng Li is a PhD candidate in Computer Science at the University of Illinois at Urbana-Champaign (UIUC) and a member of the IMPACT Research Group led by Professor Wen-Mei Hwu. Her research lies in the field of GPU-accelerated applications, with an emphasis on Deep Learning. Her work has focused on understanding and optimizing Deep Learning workloads. Before UIUC, she received her MS degree in Computer Science and Engineering and BS degree in Computer Engineering from University of Michigan, and another BS in Electrical Engineering from Shanghai Jiao Tong University. -->

